{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ccd71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary requirements: # python3, numpy, pandas, xarray, dask, bottleneck, netCDF4, cftime, matplotlib, marineHeatWaves\n",
    "# run this script to check if all dependencies are installed and working\n",
    "# also fixes np.NaN to np.nan in marineHeatWaves module\n",
    "\n",
    "import sys, platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr, dask, bottleneck, netCDF4, cftime\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, Dict, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib.dates as mdates\n",
    "import marineHeatWaves as mhw, re, pathlib\n",
    "from importlib.metadata import version, PackageNotFoundError\n",
    "\n",
    "# convert np.NAN to np.nan to avoid dependency issues\n",
    "if not hasattr(np, \"NaN\"):\n",
    "    np.NaN = np.nan  \n",
    "\n",
    "#Check the package is working\n",
    "try:\n",
    "    mhw_ver = version('marineHeatWaves')\n",
    "except PackageNotFoundError:\n",
    "    mhw_ver = getattr(mhw, '__version__', 'unknown')\n",
    "print('OK:', pd.__version__, xr.__version__, dask.__version__, 'mhw:', mhw_ver)\n",
    "print('mhw module file:', getattr(mhw, '__file__', 'unknown'))\n",
    "\n",
    "#checkk the marineHeatWaves module file and replace np.NaN with np.nan\n",
    "p = pathlib.Path(mhw.__file__)\n",
    "p.write_text(re.sub(r'\\bnp\\.NaN\\b', 'np.nan', p.read_text()))\n",
    "\n",
    "print(sys.executable)\n",
    "print(platform.python_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ece2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if marineHeatWaves package is working\n",
    "\n",
    "t = pd.date_range(\"2000-01-01\", periods=40, freq=\"D\")\n",
    "ords = np.array([d.toordinal() for d in t], dtype=int)\n",
    "temp = 25 + np.sin(np.linspace(0, 6.28, 40)); temp[15:25] += 2\n",
    "\n",
    "res, clim = mhw.detect(ords, temp, climatologyPeriod=[2000, 2000], pctile=90, minDuration=5, joinAcrossGaps=True) #hobdey's defination\n",
    "print(\"events:\", len(res.get(\"time_start\", [])))\n",
    "\n",
    "# Verifies that core libs are importable and compatible with marineHeatWaves.\n",
    "from importlib import import_module\n",
    "import numpy as np\n",
    "import marineHeatWaves as mhw\n",
    "\n",
    "want = [\"numpy\", \"pandas\", \"xarray\", \"dask\", \"bottleneck\",\"netCDF4\", \"cftime\", \"marineHeatWaves\"]\n",
    "vers = {}\n",
    "problems = []\n",
    "\n",
    "for m in want:\n",
    "    try:\n",
    "        import_module(m)\n",
    "        try:\n",
    "            vers[m] = version(m)\n",
    "        except PackageNotFoundError:\n",
    "            mod = import_module(m)\n",
    "            vers[m] = getattr(mod, \"__version__\", \"unknown\")\n",
    "    except Exception as e:\n",
    "        problems.append((m, str(e)))\n",
    "        vers[m] = \"IMPORT FAILED\"\n",
    "\n",
    "print(\"Package versions detailes:\")\n",
    "for k in want:\n",
    "    print(f\"{k:16s} : {vers[k]}\")\n",
    "if problems:\n",
    "    print(\"\\n[ENV WARNING] Some packages failed to import:\")\n",
    "    for m, msg in problems:\n",
    "        print(f\" - {m}: {msg}\")\n",
    "\n",
    "# marineHeatWaves.detect accepts ordinal ints + 1D temp array\n",
    "try:\n",
    "    # make a synthetic 40-day series with a warm spell\n",
    "    days = np.array([pd.Timestamp(\"2000-01-01\") + pd.Timedelta(i, \"D\") for i in range(40)])\n",
    "    ords = np.array([d.toordinal() for d in days], dtype=int)\n",
    "    temp = 25 + np.sin(np.linspace(0, 6.28, 40))\n",
    "    temp[15:25] += 2.0  # warm event\n",
    "    res, clim = mhw.detect(ords, temp, climatologyPeriod=[1991, 2020], pctile=90, minDuration=5, joinAcrossGaps=True)\n",
    "    print(\"\\nmarineHeatWaves.detect is working correctly.\")\n",
    "    print(f\"Detected events: {len(res.get('time_start', []))}\")\n",
    "except Exception as e:\n",
    "    print(\"\\n[ENV ERROR] marineHeatWaves.detect detection failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9280ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the files and Summarize the data\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# file path: data loading and visualization\n",
    "FILES_GLOB = \"/home/Desktop/Noah_data_1982-2024_SST_daily_mean/sst.day.mean.*.nc\"\n",
    "\n",
    "# region of interest\n",
    "ROI = dict(lat_min=0.0, lat_max=30.0, lon_min=40.0, lon_max=110.0)\n",
    "\n",
    "# North indian ocean subregions\n",
    "REGIONS = {\n",
    "    \"Arabian Sea\":    {\"lon_min\": 40.0, \"lon_max\": 78.0,  \"lat_min\": 0.0, \"lat_max\": 30.0},\n",
    "    \"Bay Of Bengal\":   {\"lon_min\": 78.0, \"lon_max\": 110.0, \"lat_min\": 0.0, \"lat_max\": 30.0},\n",
    "    \"North Indian Ocean\": {\"lon_min\": 40.0, \"lon_max\": 110.0, \"lat_min\": 0.0, \"lat_max\": 30.0},\n",
    "}\n",
    "\n",
    "# IO / plotting\n",
    "use_dask = True                 # set False to fully load into memory\n",
    "chunks = {\"time\": 90} if use_dask else None\n",
    "sample_map_date: Optional[str] = None  \n",
    "boxmean_var_name = \"sst\"       \n",
    "climatologyPeriod = [1982, 2024]  \n",
    "\n",
    "# loading the data\n",
    "def open_sst(files, chunks=None, engine: str = \"netcdf4\") -> xr.Dataset:\n",
    "    if isinstance(files, str):\n",
    "        p = Path(files)\n",
    "        if any(ch in files for ch in \"*?[]\"):\n",
    "            paths = sorted(glob(files))\n",
    "            if not paths:\n",
    "                raise FileNotFoundError(f\"No files match glob: {files}\")\n",
    "            return xr.open_mfdataset(paths, combine=\"by_coords\", parallel=True, chunks=chunks, engine=engine)\n",
    "        else:\n",
    "            if not p.exists():\n",
    "                raise FileNotFoundError(f\"File not found: {files}\")\n",
    "            return xr.open_dataset(files, chunks=chunks, engine=engine)\n",
    "    else:\n",
    "        paths = [str(Path(f)) for f in files]\n",
    "        for f in paths:\n",
    "            if not Path(f).exists():\n",
    "                raise FileNotFoundError(f\"File not found: {f}\")\n",
    "        return xr.open_mfdataset(paths, combine=\"by_coords\", parallel=True, chunks=chunks, engine=engine)\n",
    "\n",
    "# Subset to region of interest\n",
    "def subset_roi(ds: xr.Dataset, roi: Dict[str, float], var: str = \"sst\") -> xr.Dataset:\n",
    "    # Make sure coords are named commonly\n",
    "    lat_name = \"lat\" if \"lat\" in ds.coords else \"latitude\"\n",
    "    lon_name = \"lon\" if \"lon\" in ds.coords else \"longitude\"\n",
    "\n",
    "    ds2 = ds.sel(**{lat_name: slice(roi[\"lat_min\"], roi[\"lat_max\"]),lon_name: slice(roi[\"lon_min\"], roi[\"lon_max\"]),})\n",
    "    # Keep only the variable of interest and coords\n",
    "    if var in ds2:\n",
    "        return ds2[[var]]\n",
    "    else:\n",
    "        raise KeyError(f\"Variable '{var}' not found. Available: {list(ds2.data_vars)}\")\n",
    "\n",
    "# Print metadata\n",
    "def print_metadata(ds: xr.Dataset, var: str = \"sst\") -> None:\n",
    "    # Dimention sizes\n",
    "    for c in ds.coords:\n",
    "        arr = ds.coords[c]\n",
    "        try:\n",
    "            vals = arr.values\n",
    "            preview = f\"{vals[:3]} ... {vals[-3:]}\" if vals.size > 6 else vals\n",
    "        except Exception:\n",
    "            preview = arr\n",
    "        print(f\"{c}: {preview}\")\n",
    "\n",
    "    # convert time \n",
    "    if \"time\" in ds.coords:\n",
    "        t = pd.to_datetime(ds[\"time\"].values)\n",
    "        print(\"\\nTime Coverage:\")\n",
    "        print(f\"Start: {pd.Timestamp(t[0]).date()}, End: {pd.Timestamp(t[-1]).date()}, Length: {t.size} steps\")\n",
    "\n",
    "    # Global attrs\n",
    "    print(\"\\nGlobal attributes:\")\n",
    "    for k, v in ds.attrs.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    # Variable attrs\n",
    "    if var in ds:\n",
    "        print(f\"\\nVariable '{var}' attributes:\")\n",
    "        for k, v in ds[var].attrs.items():\n",
    "            print(f\"{k}: {v}\") \n",
    "            print(\"Dimensions:\")\n",
    "    for k, v in ds.dims.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    # Coordinates preview\n",
    "    print(\"\\nCoordinates (first few):\")\n",
    "   \n",
    "def summarize_events_table(res: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert marineHeatWaves 'res' dict to a tidy event table.'time_*' in res are *ordinal* days (Python datetime.toordinal).\n",
    "    \"\"\"\n",
    "    to_ts = lambda arr: pd.to_datetime([pd.Timestamp.fromordinal(int(d)) for d in np.asarray(arr)])\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"start_date\":                   to_ts(res[\"time_start\"]),\n",
    "        \"end_date\":                     to_ts(res[\"time_end\"]),\n",
    "        \"duration_days\":                res[\"duration\"],\n",
    "        \"intensity_max_degC\":           res[\"intensity_max\"],\n",
    "        \"intensity_mean_degC\":          res[\"intensity_mean\"],\n",
    "        \"cumulative_intensity_degC\":    res[\"intensity_cumulative\"],\n",
    "    })\n",
    "\n",
    "# Open files\n",
    "ds = open_sst(FILES_GLOB, chunks={\"time\": 120}, engine=\"netcdf4\")\n",
    "\n",
    "# Subset ROI &  'sst'\n",
    "ds_roi = subset_roi(ds, ROI, var=boxmean_var_name)\n",
    "da = ds_roi[boxmean_var_name]\n",
    "\n",
    "# Inspect metadata\n",
    "print_metadata(ds_roi, var=boxmean_var_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c3fa7",
   "metadata": {},
   "source": [
    "# Common script to calculate the marine heat wave for Arabian Sea, Bay of Bengal and North India Ocean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be442e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common script to calculate the marine heat wave \n",
    "# Imports the libraries and dependencies\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Oliver’s package (Hobday method implementation)\n",
    "import marineHeatWaves as mhw\n",
    "\n",
    "# files path\n",
    "FILES_GLOB = \"/home/Noah_data_1982-2024_SST_daily_mean/sst.day.mean.*.nc\" \n",
    "SST_VAR    = \"sst\"\n",
    "\n",
    "# define the baseline year or analysis time-frame\n",
    "BASELINE = (1982, 2024)\n",
    "\n",
    "# Event definition based on Hobday et al. (2016)\n",
    "MIN_DUR  = 5     # minimum event duration (days)\n",
    "MAX_GAP  = 2     # join across gaps up to this many days\n",
    "\n",
    "# Dask-friendly chunks\n",
    "CHTIME, CHXY = 160, 40\n",
    "\n",
    "# Set the regions\n",
    "ROI_DICT = {\n",
    "    \"Arabian Sea\": {\"lon_min\": 40.0, \"lon_max\": 80.0, \"lat_min\":  0.0, \"lat_max\": 30.0, \"slug\": \"arabian_sea\"},\n",
    "    \"Bay Of Bengal\": {\"lon_min\": 80.0, \"lon_max\": 110.0,\"lat_min\":  0.0, \"lat_max\":  30.0, \"slug\": \"bay_of_bengal\"},\n",
    "    \"North Indian Ocean\": {\"lon_min\": 40.0, \"lon_max\": 110.0,\"lat_min\":  0.0, \"lat_max\":  30.0,\"slug\": \"north_indian_ocean\"},\n",
    "}\n",
    "\n",
    "OUTROOT = Path(\"outputs_mhw\"); OUTROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# open SST data using xarray\n",
    "def open_sst(files_glob: str, roi: dict) -> tuple[xr.Dataset, str, str]:\n",
    "    \"\"\"Open OISST, subset ROI, return dataset and coordinate names.\"\"\"\n",
    "    paths = sorted(glob(files_glob))\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No files match: {files_glob}\")\n",
    "    ds = xr.open_mfdataset(paths, combine=\"by_coords\",chunks={\"time\": CHTIME}, engine=\"netcdf4\")\n",
    "    latn = \"lat\" if \"lat\" in ds.coords else \"latitude\"\n",
    "    lonn = \"lon\" if \"lon\" in ds.coords else \"longitude\"\n",
    "    ds = ds.sel({latn: slice(roi[\"lat_min\"], roi[\"lat_max\"]),lonn: slice(roi[\"lon_min\"], roi[\"lon_max\"])})\n",
    "    return ds, latn, lonn\n",
    "# calculate the area of the grids\n",
    "def area_weights_1d(ds: xr.Dataset, latn: str) -> xr.DataArray:\n",
    "    \"\"\"1-D cos(lat) weights (finite) broadcastable across lon.\"\"\"\n",
    "    return np.cos(np.deg2rad(ds[latn]))\n",
    "\n",
    "# use the hobdey's defination on region\n",
    "def _detect_mask_1d(sst_1d: np.ndarray, thresh_1d: np.ndarray, min_dur: int = MIN_DUR, max_gap: int = MAX_GAP) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Hobday event mask on a single time series: exceed >= threshold; join gaps <= max_gap; drop runs < min_dur.\n",
    "    \"\"\"\n",
    "    ok = np.isfinite(sst_1d) & np.isfinite(thresh_1d)\n",
    "    exc = ok & (sst_1d >= thresh_1d)\n",
    "    if not exc.any():\n",
    "        return exc\n",
    "    x = exc.astype(np.int8)\n",
    "    n = x.size\n",
    "\n",
    "    # join short gaps\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        if x[i] == 1:\n",
    "            j = i + 1\n",
    "            while j < n and x[j] == 1:\n",
    "                j += 1\n",
    "            g0 = j\n",
    "            while g0 < n and x[g0] == 0:\n",
    "                g0 += 1\n",
    "            gap_len = g0 - j\n",
    "            if gap_len > 0 and gap_len <= max_gap:\n",
    "                x[j:g0] = 1\n",
    "                j = g0\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    # enforce min duration\n",
    "    y = x.copy()\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        if y[i] == 1:\n",
    "            j = i + 1\n",
    "            while j < n and y[j] == 1:\n",
    "                j += 1\n",
    "            if (j - i) < min_dur:\n",
    "                y[i:j] = 0\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return y.astype(bool)\n",
    "\n",
    "def detect_mask_time(sst: xr.DataArray, thresh: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\" Vectorized Hobday event mask over (time, lat, lon). Requires single time-chunk.\"\"\"\n",
    "    return xr.apply_ufunc(_detect_mask_1d, sst, thresh, input_core_dims=[[\"time\"], [\"time\"]], output_core_dims=[[\"time\"]],\n",
    "                          vectorize=True, dask=\"parallelized\",output_dtypes=[bool],)\n",
    "\n",
    "# per grid climatology & threshold \n",
    "def _clim_thresh_time_1d(ords_1d: np.ndarray, temp_1d: np.ndarray,y0: int, y1: int, pct: int,\n",
    "                         min_dur: int, max_gap: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    For one grid time series: run mhw.detect to get time-aligned climatology (seas) and threshold.\n",
    "    \"\"\"\n",
    "    # If mostly missing, return NaNs to avoid unstable fits\n",
    "    if np.isfinite(temp_1d).sum() < 30:\n",
    "        n = temp_1d.size\n",
    "        return np.full(n, np.nan, float), np.full(n, np.nan, float)\n",
    "\n",
    "    # oliver's detection\n",
    "    _, clim = mhw.detect(ords_1d.astype(int), temp_1d.astype(float),climatologyPeriod=[int(y0), int(y1)],pctile=int(pct),minDuration=int(min_dur),\n",
    "        joinAcrossGaps=True,maxGap=int(max_gap),)\n",
    "    seas   = np.asarray(clim[\"seas\"],   float)\n",
    "    thresh = np.asarray(clim[\"thresh\"], float)\n",
    "    return seas, thresh\n",
    "                             \n",
    "# compute time-aligned climatology and threshold for every grid\n",
    "def build_grid_baseline(ds: xr.Dataset, latn: str, lonn: str,pctile: int = 90) -> tuple[xr.DataArray, xr.DataArray]:\n",
    "    \n",
    "    t_index = pd.to_datetime(ds[\"time\"].values)\n",
    "    y0 = max(BASELINE[0], t_index.year.min())\n",
    "    y1 = min(BASELINE[1], t_index.year.max())\n",
    "    ords_da = xr.DataArray(np.array([d.toordinal() for d in t_index], dtype=int),coords={\"time\": ds[\"time\"]},dims=[\"time\"],).chunk({\"time\": -1})  # single time chunk for gufunc\n",
    "    sst = ds[SST_VAR].chunk({\"time\": -1, latn: CHXY, lonn: CHXY})\n",
    "\n",
    "    seas_t, thresh_t = xr.apply_ufunc(_clim_thresh_time_1d, ords_da, sst,input_core_dims=[[\"time\"], [\"time\"]],output_core_dims=[[\"time\"], [\"time\"]],\n",
    "        vectorize=True, dask=\"parallelized\",output_dtypes=[float, float],kwargs=dict(y0=int(y0), y1=int(y1), pct=int(pctile),min_dur=MIN_DUR, max_gap=MAX_GAP),)\n",
    "    seas_t   = seas_t.rename(\"seas_t\")\n",
    "    thresh_t = thresh_t.rename(\"thresh_t\")\n",
    "    return seas_t, thresh_t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9eab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "REG = \"Arabian Sea\"\n",
    "ROI = ROI_DICT[REG]\n",
    "\n",
    "# Use None or 0 to show every year.\n",
    "YEAR_TICK_STEP  = 5\n",
    "# (Optional) force a start/end for ticks; leave as None to auto-fit data range\n",
    "YEAR_TICK_START = None\n",
    "YEAR_TICK_END   = None\n",
    "# (Optional) label rotation (degrees)\n",
    "YEAR_TICK_ROT   = 0\n",
    "\n",
    "def apply_year_ticks(ax, years, step=YEAR_TICK_STEP, start=YEAR_TICK_START, end=YEAR_TICK_END, rotate=YEAR_TICK_ROT):\n",
    "    years = np.asarray(years, dtype=int)\n",
    "    y_min, y_max = int(years.min()), int(years.max())\n",
    "    a = y_min if start is None else int(start)\n",
    "    b = y_max if end   is None else int(end)\n",
    "    if (step is None) or (step == 0):\n",
    "        ticks = np.unique(years)\n",
    "    else:\n",
    "        ticks = np.arange(a, b + 1, int(step), dtype=int)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels([str(y) for y in ticks], rotation=rotate)\n",
    "\n",
    "# Open SST (ROI)\n",
    "ds, latn, lonn = open_sst(FILES_GLOB, ROI)\n",
    "\n",
    "# Land mask & chunks\n",
    "ocean = ds[SST_VAR].notnull().any(\"time\")\n",
    "sst   = ds[SST_VAR].where(ocean).chunk({\"time\": CHTIME, latn: CHXY, lonn: CHXY})\n",
    "\n",
    "# Per-grid climatology & 90th-threshold (Oliver/Hobday) — time-aligned\n",
    "seas_t, thresh_t = build_grid_baseline(ds, latn, lonn, pctile=90)\n",
    "\n",
    "# Detect Hobday events per grid (ensure single time chunk along 'time')\n",
    "# compute boolean mask for logic\n",
    "evt_mask_bool = detect_mask_time(sst.chunk({\"time\": -1}),thresh_t.chunk({\"time\": -1})).rename(\"mhw_mask\")  # bool\n",
    "# NaN on land & keep float so NaNs persist to Zarr\n",
    "evt_mask = evt_mask_bool.where(ocean).astype(\"float32\")\n",
    "\n",
    "# Daily metrics for future analysis\n",
    "intensity = (sst - seas_t).rename(\"intensity\") \n",
    "excess    = (sst - thresh_t).where(evt_mask == 1).rename(\"excess\")  # only on event days\n",
    "\n",
    "# Yearly per-grid summaries\n",
    "# compute starts on the boolean mask\n",
    "starts_bool = (evt_mask_bool & ~(evt_mask_bool.shift(time=1, fill_value=False)))\n",
    "\n",
    "# NaN on land, keep float dtype \n",
    "starts = starts_bool.where(ocean).astype(\"float32\")\n",
    "events_per_year_grid = (starts.groupby(\"time.year\").sum(\"time\").rename(\"events_per_year\").astype(\"float32\"))  \n",
    "days_per_year_grid = (evt_mask.groupby(\"time.year\").sum(\"time\").rename(\"days_per_year\").astype(\"float32\"))\n",
    "\n",
    "# Area-weighted regional means (per year)\n",
    "w_lat = area_weights_1d(ds, latn)\n",
    "freq_region = (events_per_year_grid.where(ocean)).weighted(w_lat).mean(dim=[latn, lonn]).compute()\n",
    "days_region = (days_per_year_grid.where(ocean)).weighted(w_lat).mean(dim=[latn, lonn]).compute()\n",
    "total_events_region = float(freq_region.sum().values)\n",
    "total_days_region   = float(days_region.sum().values)\n",
    "print(f\"{REG} totals — events: {total_events_region:.1f}, days: {total_days_region:.1f}\")\n",
    "\n",
    "# Quick plots (region-mean series)\n",
    "years_freq = (freq_region.coords.get(\"year\", None).values\n",
    "              if \"year\" in freq_region.coords\n",
    "              else freq_region.get_index(\"year\").values).astype(int)\n",
    "years_days = (days_region.coords.get(\"year\", None).values\n",
    "              if \"year\" in days_region.coords\n",
    "              else days_region.get_index(\"year\").values).astype(int)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.bar(years_freq, freq_region.values, color=\"#5b8def\", width=0.8)\n",
    "ax.set_title(f\" MHW events counts in {REG} {BASELINE[0]}–{BASELINE[1]}\")\n",
    "ax.set_xlabel(\"Year\"); ax.set_ylabel(\"Events/year\")\n",
    "ax.grid(True, ls=\"--\", alpha=0.4)\n",
    "ax.text(0.5, 0.92, f\"Total Events: {total_events_region:.1f}\", transform=ax.transAxes, ha=\"center\")\n",
    "apply_year_ticks(ax, years_freq)  \n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.bar(years_days, days_region.values, color=\"#f6a300\", width=0.8)\n",
    "ax.set_title(f\"MHW days in {REG} {BASELINE[0]}–{BASELINE[1]}\")\n",
    "ax.set_xlabel(\"Year\"); ax.set_ylabel(\"Days/year\")\n",
    "ax.grid(True, ls=\"--\", alpha=0.4)\n",
    "ax.text(0.5, 0.92, f\"Total MHW days: {total_days_region:.1f}\", transform=ax.transAxes, ha=\"center\")\n",
    "apply_year_ticks(ax, years_days)  \n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 9) SAVE (Zarr + CSV)\n",
    "out_dir = OUTROOT / ROI_DICT[REG][\"slug\"]; out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "daily_ds = xr.Dataset({\"mhw_mask\": evt_mask, \"intensity\": intensity, \"excess\": excess, \"seas_t\": seas_t, \"thresh_t\": thresh_t},\n",
    "    coords={\"time\": ds[\"time\"], latn: ds[latn], lonn: ds[lonn]},).chunk({\"time\": CHTIME, latn: CHXY, lonn: CHXY})\n",
    "\n",
    "# Keep only core coords to avoid Zarr alignment issues\n",
    "keep = {\"time\", latn, lonn}\n",
    "dropc = [c for c in daily_ds.coords if c not in keep]\n",
    "if dropc:\n",
    "    daily_ds = daily_ds.reset_coords(dropc, drop=True)\n",
    "\n",
    "print(\"Writing Zarr outputs… (can take time)\")\n",
    "with ProgressBar():\n",
    "    daily_ds.to_zarr(out_dir / \"mhw_daily.zarr\", mode=\"w\", safe_chunks=False, align_chunks=True)\n",
    "    events_per_year_grid.to_zarr(out_dir / \"mhw_events_per_year_grid.zarr\", mode=\"w\", safe_chunks=False, align_chunks=True)\n",
    "    days_per_year_grid.to_zarr(out_dir / \"mhw_days_per_year_grid.zarr\", mode=\"w\", safe_chunks=False, align_chunks=True)\n",
    "\n",
    "freq_region.to_dataframe().to_csv(out_dir / \"mhw_region_mean_events_per_year.csv\")\n",
    "days_region.to_dataframe().to_csv(out_dir / \"mhw_region_mean_days_per_year.csv\")\n",
    "pd.DataFrame({\"mhw_total_events_region_mean\":[total_events_region],\"mhw_total_days_region_mean\":[total_days_region]}).to_csv(out_dir / \"mhw_region_totals.csv\", index=False)\n",
    "\n",
    "print(\"Arabian Sea — done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726906e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "REG = \"Bay Of Bengal\"\n",
    "ROI = ROI_DICT[REG]\n",
    "\n",
    "# Use None or 0 to show every year.\n",
    "YEAR_TICK_STEP  = 5\n",
    "# (Optional) force a start/end for ticks\n",
    "YEAR_TICK_START = None\n",
    "YEAR_TICK_END   = None\n",
    "# (Optional) label rotation (degrees)\n",
    "YEAR_TICK_ROT   = 0\n",
    "\n",
    "def apply_year_ticks(ax, years, step=YEAR_TICK_STEP, start=YEAR_TICK_START, end=YEAR_TICK_END, rotate=YEAR_TICK_ROT):\n",
    "    years = np.asarray(years, dtype=int)\n",
    "    y_min, y_max = int(years.min()), int(years.max())\n",
    "    a = y_min if start is None else int(start)\n",
    "    b = y_max if end   is None else int(end)\n",
    "    if (step is None) or (step == 0):\n",
    "        ticks = np.unique(years)\n",
    "    else:\n",
    "        ticks = np.arange(a, b + 1, int(step), dtype=int)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels([str(y) for y in ticks], rotation=rotate)\n",
    "\n",
    "# Open SST (ROI)\n",
    "ds, latn, lonn = open_sst(FILES_GLOB, ROI)\n",
    "\n",
    "# Land mask & chunks\n",
    "ocean = ds[SST_VAR].notnull().any(\"time\")\n",
    "sst   = ds[SST_VAR].where(ocean).chunk({\"time\": CHTIME, latn: CHXY, lonn: CHXY})\n",
    "\n",
    "# per-grid climatology & 90th-threshold \n",
    "seas_t, thresh_t = build_grid_baseline(ds, latn, lonn, pctile=90)\n",
    "\n",
    "# detect events per grid \n",
    "# CHANGED: compute boolean mask for logic\n",
    "evt_mask_bool = detect_mask_time(sst.chunk({\"time\": -1}),thresh_t.chunk({\"time\": -1})).rename(\"mhw_mask\")  # bool\n",
    "\n",
    "# NaN on land & keep float so NaNs persist to Zarr\n",
    "evt_mask = evt_mask_bool.where(ocean).astype(\"float32\")\n",
    "\n",
    "# daily metrics for future analysis\n",
    "intensity = (sst - seas_t).rename(\"intensity\")                 # all days\n",
    "excess    = (sst - thresh_t).where(evt_mask == 1).rename(\"excess\")  # only on event days\n",
    "\n",
    "# yearly per-grid summaries\n",
    "# compute starts on the boolean mask\n",
    "starts_bool = (evt_mask_bool &~(evt_mask_bool.shift(time=1, fill_value=False)))\n",
    "\n",
    "# NaN on land, keep float dtype\n",
    "starts = starts_bool.where(ocean).astype(\"float32\")\n",
    "events_per_year_grid = (starts.groupby(\"time.year\").sum(\"time\").rename(\"events_per_year\").astype(\"float32\"))\n",
    "days_per_year_grid = (evt_mask  .groupby(\"time.year\").sum(\"time\").rename(\"days_per_year\").astype(\"float32\"))\n",
    "\n",
    "# area-weighted regional means (per year)\n",
    "w_lat = area_weights_1d(ds, latn)\n",
    "freq_region = (events_per_year_grid.where(ocean)).weighted(w_lat).mean(dim=[latn, lonn]).compute()\n",
    "days_region = (days_per_year_grid.where(ocean)).weighted(w_lat).mean(dim=[latn, lonn]).compute()\n",
    "\n",
    "total_events_region = float(freq_region.sum().values)\n",
    "total_days_region   = float(days_region.sum().values)\n",
    "print(f\"{REG} totals — events: {total_events_region:.1f}, days: {total_days_region:.1f}\")\n",
    "\n",
    "# plot the graphs (region-mean series)\n",
    "years_freq = (freq_region.coords.get(\"year\", None).values\n",
    "              if \"year\" in freq_region.coords\n",
    "              else freq_region.get_index(\"year\").values).astype(int)\n",
    "years_days = (days_region.coords.get(\"year\", None).values\n",
    "              if \"year\" in days_region.coords\n",
    "              else days_region.get_index(\"year\").values).astype(int)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.bar(years_freq, freq_region.values, color=\"#5b8def\", width=0.8)\n",
    "ax.set_title(f\" MHW events counts in {REG} {BASELINE[0]}–{BASELINE[1]}\")\n",
    "ax.set_xlabel(\"Year\"); ax.set_ylabel(\"Events/year\")\n",
    "ax.grid(True, ls=\"--\", alpha=0.4)\n",
    "ax.text(0.5, 0.92, f\"Total Events: {total_events_region:.1f}\", transform=ax.transAxes, ha=\"center\")\n",
    "apply_year_ticks(ax, years_freq)   \n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.bar(years_days, days_region.values, color=\"#f6a300\", width=0.8)\n",
    "ax.set_title(f\"MHW days in {REG} {BASELINE[0]}–{BASELINE[1]}\")\n",
    "ax.set_xlabel(\"Year\"); ax.set_ylabel(\"Days/year\")\n",
    "ax.grid(True, ls=\"--\", alpha=0.4)\n",
    "ax.text(0.5, 0.92, f\"Total MHW days: {total_days_region:.1f}\", transform=ax.transAxes, ha=\"center\")\n",
    "apply_year_ticks(ax, years_days)   \n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# saveing the zarr and csv files\n",
    "out_dir = OUTROOT / ROI_DICT[REG][\"slug\"]; out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "daily_ds = xr.Dataset({\"mhw_mask\": evt_mask, \"intensity\": intensity, \"excess\": excess,\"seas_t\": seas_t, \"thresh_t\": thresh_t},\n",
    "    coords={\"time\": ds[\"time\"], latn: ds[latn], lonn: ds[lonn]},).chunk({\"time\": CHTIME, latn: CHXY, lonn: CHXY})\n",
    "\n",
    "# Keep only core coords to avoid Zarr alignment issues\n",
    "keep = {\"time\", latn, lonn}\n",
    "dropc = [c for c in daily_ds.coords if c not in keep]\n",
    "if dropc:daily_ds = daily_ds.reset_coords(dropc, drop=True)\n",
    "\n",
    "print(\"Writing Zarr outputs… (can take time)\")\n",
    "with ProgressBar():\n",
    "    daily_ds.to_zarr(out_dir / \"mhw_daily.zarr\", mode=\"w\",safe_chunks=False, align_chunks=True)\n",
    "    events_per_year_grid.to_zarr(out_dir / \"mhw_events_per_year_grid.zarr\", mode=\"w\",safe_chunks=False, align_chunks=True)\n",
    "    days_per_year_grid.to_zarr(out_dir / \"mhw_days_per_year_grid.zarr\", mode=\"w\",safe_chunks=False, align_chunks=True)\n",
    "\n",
    "freq_region.to_dataframe().to_csv(out_dir / \"mhw_region_mean_events_per_year.csv\")\n",
    "days_region.to_dataframe().to_csv(out_dir / \"mhw_region_mean_days_per_year.csv\")\n",
    "pd.DataFrame({\"mhw_total_events_region_mean\":[total_events_region],\"mhw_total_days_region_mean\":[total_days_region]}).to_csv(out_dir / \"mhw_region_totals.csv\", index=False)\n",
    "print(\"Bay of Bengal — done.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f8db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "REG = \"North Indian Ocean\"\n",
    "ROI = ROI_DICT[REG]\n",
    " \n",
    "# Use None or 0 to show every year.\n",
    "YEAR_TICK_STEP  = 5\n",
    "# (Optional) force a start/end for ticks; leave as None to auto-fit data range\n",
    "YEAR_TICK_START = None\n",
    "YEAR_TICK_END   = None\n",
    "# (Optional) label rotation (degrees)\n",
    "YEAR_TICK_ROT   = 0\n",
    "\n",
    "def apply_year_ticks(ax, years, step=YEAR_TICK_STEP, start=YEAR_TICK_START, end=YEAR_TICK_END, rotate=YEAR_TICK_ROT):\n",
    "    years = np.asarray(years, dtype=int)\n",
    "    y_min, y_max = int(years.min()), int(years.max())\n",
    "    a = y_min if start is None else int(start)\n",
    "    b = y_max if end   is None else int(end)\n",
    "    if (step is None) or (step == 0):\n",
    "        ticks = np.unique(years)\n",
    "    else:\n",
    "        ticks = np.arange(a, b + 1, int(step), dtype=int)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels([str(y) for y in ticks], rotation=rotate)\n",
    "\n",
    "# Open SST (ROI)\n",
    "ds, latn, lonn = open_sst(FILES_GLOB, ROI)\n",
    "\n",
    "# Land mask & chunks\n",
    "ocean = ds[SST_VAR].notnull().any(\"time\")\n",
    "sst   = ds[SST_VAR].where(ocean).chunk({\"time\": CHTIME, latn: CHXY, lonn: CHXY})\n",
    "\n",
    "# Per-grid climatology & 90th-threshold (Oliver/Hobday)\n",
    "seas_t, thresh_t = build_grid_baseline(ds, latn, lonn, pctile=90)\n",
    "\n",
    "# Detect Hobday events per grid \n",
    "# compute boolean mask for logic\n",
    "evt_mask_bool = detect_mask_time(sst.chunk({\"time\": -1}),thresh_t.chunk({\"time\": -1})).rename(\"mhw_mask\")  # bool\n",
    "\n",
    "# CHANGED: NaN on land & keep float so NaNs persist to Zarr\n",
    "evt_mask = evt_mask_bool.where(ocean).astype(\"float32\")\n",
    "\n",
    "# Daily metrics for future analysis\n",
    "intensity = (sst - seas_t).rename(\"intensity\")                \n",
    "excess    = (sst - thresh_t).where(evt_mask == 1).rename(\"excess\") \n",
    "# Yearly per-grid summaries\n",
    "starts_bool = (evt_mask_bool & ~(evt_mask_bool.shift(time=1, fill_value=False)))\n",
    "\n",
    "# NaN on land, keep float dtype\n",
    "starts = starts_bool.where(ocean).astype(\"float32\")\n",
    "events_per_year_grid = (starts.groupby(\"time.year\").sum(\"time\").rename(\"events_per_year\").astype(\"float32\")) \n",
    "days_per_year_grid = (evt_mask.groupby(\"time.year\").sum(\"time\").rename(\"days_per_year\").astype(\"float32\"))\n",
    "\n",
    "# Area-weighted regional means (per year)\n",
    "w_lat = area_weights_1d(ds, latn)\n",
    "freq_region = (events_per_year_grid.where(ocean)).weighted(w_lat).mean(dim=[latn, lonn]).compute()\n",
    "days_region = (days_per_year_grid.where(ocean)).weighted(w_lat).mean(dim=[latn, lonn]).compute()\n",
    "\n",
    "total_events_region = float(freq_region.sum().values)\n",
    "total_days_region   = float(days_region.sum().values)\n",
    "print(f\"{REG} totals — events: {total_events_region:.1f}, days: {total_days_region:.1f}\")\n",
    "\n",
    "# Quick plots\n",
    "years_freq = (freq_region.coords.get(\"year\", None).values\n",
    "              if \"year\" in freq_region.coords\n",
    "              else freq_region.get_index(\"year\").values).astype(int)\n",
    "years_days = (days_region.coords.get(\"year\", None).values\n",
    "              if \"year\" in days_region.coords\n",
    "              else days_region.get_index(\"year\").values).astype(int)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.bar(years_freq, freq_region.values, color=\"#5b8def\", width=0.8)\n",
    "ax.set_title(f\" MHW events counts in {REG} {BASELINE[0]}–{BASELINE[1]}\")\n",
    "ax.set_xlabel(\"Year\"); ax.set_ylabel(\"Events/year\")\n",
    "ax.grid(True, ls=\"--\", alpha=0.4)\n",
    "ax.text(0.5, 0.92, f\"Total Events: {total_events_region:.1f}\", transform=ax.transAxes, ha=\"center\")\n",
    "apply_year_ticks(ax, years_freq)  \n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.bar(years_days, days_region.values, color=\"#f6a300\", width=0.8)\n",
    "ax.set_title(f\"MHW days in {REG} {BASELINE[0]}–{BASELINE[1]}\")\n",
    "ax.set_xlabel(\"Year\"); ax.set_ylabel(\"Days/year\")\n",
    "ax.grid(True, ls=\"--\", alpha=0.4)\n",
    "ax.text(0.5, 0.92, f\"Total MHW days: {total_days_region:.1f}\", transform=ax.transAxes, ha=\"center\")\n",
    "apply_year_ticks(ax, years_days)  \n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# SAVE (Zarr + CSV)\n",
    "out_dir = OUTROOT / ROI_DICT[REG][\"slug\"]; out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "daily_ds = xr.Dataset({\"mhw_mask\": evt_mask, \"intensity\": intensity, \"excess\": excess,\"seas_t\": seas_t, \"thresh_t\": thresh_t},\n",
    "    coords={\"time\": ds[\"time\"], latn: ds[latn], lonn: ds[lonn]},).chunk({\"time\": CHTIME, latn: CHXY, lonn: CHXY})\n",
    "\n",
    "# Keep only core coords to avoid Zarr alignment issues\n",
    "keep = {\"time\", latn, lonn}\n",
    "dropc = [c for c in daily_ds.coords if c not in keep]\n",
    "if dropc:\n",
    "    daily_ds = daily_ds.reset_coords(dropc, drop=True)\n",
    "\n",
    "print(\"Writing Zarr outputs… (can take time)\")\n",
    "with ProgressBar():\n",
    "    daily_ds.to_zarr(out_dir / \"mhw_daily.zarr\", mode=\"w\",safe_chunks=False, align_chunks=True)\n",
    "    events_per_year_grid.to_zarr(out_dir / \"mhw_events_per_year_grid.zarr\", mode=\"w\",safe_chunks=False, align_chunks=True)\n",
    "    days_per_year_grid.to_zarr(out_dir / \"mhw_days_per_year_grid.zarr\", mode=\"w\",safe_chunks=False, align_chunks=True)\n",
    "\n",
    "freq_region.to_dataframe().to_csv(out_dir / \"mhw_region_mean_events_per_year.csv\")\n",
    "days_region.to_dataframe().to_csv(out_dir / \"mhw_region_mean_days_per_year.csv\")\n",
    "pd.DataFrame({\"mhw_total_events_region_mean\":[total_events_region],\"mhw_total_days_region_mean\":[total_days_region]}).to_csv(out_dir / \"mhw_region_totals.csv\", index=False)\n",
    "\n",
    "print(\"North Indian Ocean MHW — done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d7814b",
   "metadata": {},
   "source": [
    "# The following script compute and plot the  combined bar graph of AS, BoB and NIO regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and plot the  combined bar graph of AS, BoB and NIO regions\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# CSV Files path for event analysis\n",
    "as_event_file  = \"/home/Desktop/arabian_sea/mhw_region_mean_events_per_year.csv\"\n",
    "bob_event_file = \"/home/Desktop/bay_of_bengal/mhw_region_mean_events_per_year.csv\"\n",
    "nio_event_file = \"/home/Desktop/north_indian_ocean/mhw_region_mean_events_per_year.csv\"   #provide the proper path here\n",
    "\n",
    "# Load the csv file\n",
    "as_event_df  = pd.read_csv(as_event_file)\n",
    "bob_event_df = pd.read_csv(bob_event_file)\n",
    "nio_event_df = pd.read_csv(nio_event_file)\n",
    "\n",
    "# Rename columns\n",
    "as_event_df  = as_event_df.rename(columns={\"events_per_year\": \"AS\"})\n",
    "bob_event_df = bob_event_df.rename(columns={\"events_per_year\": \"BoB\"})\n",
    "nio_event_df = nio_event_df.rename(columns={\"events_per_year\": \"NIO\"})\n",
    "\n",
    "# merge into single dataframe\n",
    "event_df = as_event_df.merge(bob_event_df, on=\"year\").merge(nio_event_df, on=\"year\")\n",
    "event_years = event_df[\"year\"].values\n",
    "\n",
    "# Combine the regions bar into one graph\n",
    "x = np.arange(len(event_years))\n",
    "w = 0.25   # adjust the bar width\n",
    "\n",
    "# Plot the figure\n",
    "fig, ax = plt.subplots(figsize=(14,6))\n",
    "ax.bar(x - w, event_df[\"AS\"],  width=w, color=\"#0072B2\",  label=\"Arabian Sea\")   # blue \n",
    "ax.bar(x,     event_df[\"BoB\"], width=w, color= \"#D55E00\", label=\"Bay of Bengal\")  # vermillion\n",
    "ax.bar(x + w, event_df[\"NIO\"], width=w, color= \"#009E73\", label=\"North Indian Ocean\")  # bluish green\n",
    "\n",
    "ax.set_title(\"MHW Events Comparison of AS, BoB and NIO (1982–2024)\")\n",
    "ax.set_ylabel(\"Events / year\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "\n",
    "# 5-year ticks\n",
    "tick_idx = np.arange(0, len(event_years), 3)\n",
    "ax.set_xticks(tick_idx)\n",
    "ax.set_xticklabels(event_years[tick_idx])\n",
    "ax.legend()\n",
    "ax.grid(True, ls=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00088099",
   "metadata": {},
   "source": [
    "# The following Script compute and plot the  combined bar graph total MHW days of AS, BoB and NIO regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e955af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and plot the  combined bar graph total MHW days of AS, BoB and NIO regions\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# CSV Files path for days analysis\n",
    "as_day_file  = \"/home/Desktop/Jupyter files/outputs_mhw/arabian_sea/mhw_region_mean_days_per_year.csv\"\n",
    "bob_day_file = \"/home/Desktop/Jupyter files/outputs_mhw/bay_of_bengal/mhw_region_mean_days_per_year.csv\"\n",
    "nio_day_file = \"/home/Desktop/Jupyter files/outputs_mhw/north_indian_ocean/mhw_region_mean_days_per_year.csv\"   #provide the proper path here\n",
    "\n",
    "# Load the days csv file\n",
    "as_day_df  = pd.read_csv(as_day_file)\n",
    "bob_day_df = pd.read_csv(bob_day_file)\n",
    "nio_day_df = pd.read_csv(nio_day_file)\n",
    "\n",
    "# Rename columns\n",
    "as_day_df  = as_day_df.rename(columns={\"days_per_year\": \"AS\"})\n",
    "bob_day_df = bob_day_df.rename(columns={\"days_per_year\": \"BoB\"})\n",
    "nio_day_df = nio_day_df.rename(columns={\"days_per_year\": \"NIO\"})\n",
    "\n",
    "# merge into single dataframe\n",
    "day_df = as_day_df.merge(bob_day_df, on=\"year\").merge(nio_day_df, on=\"year\")\n",
    "day_years = day_df[\"year\"].values\n",
    "\n",
    "# Combine the regions bar into one graph\n",
    "x = np.arange(len(day_years))\n",
    "w = 0.25   # adjust the bar width\n",
    "\n",
    "# Plot the figures\n",
    "fig, ax = plt.subplots(figsize=(14,6))\n",
    "ax.bar(x - w, day_df[\"AS\"],  width=w, color= \"#440154\",  label=\"Arabian Sea\")    #Dark Purple\n",
    "ax.bar(x,     day_df[\"BoB\"], width=w, color= \"#21918c\", label=\"Bay of Bengal\")  # Teal\n",
    "ax.bar(x + w, day_df[\"NIO\"], width=w, color= \"#fde725\", label=\"North Indian Ocean\")  # Yellow\n",
    "\n",
    "ax.set_title(\"MHW days Comparison of AS, BoB and NIO (1982–2024)\")\n",
    "ax.set_ylabel(\" MHW Days\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "\n",
    "# 5-year ticks\n",
    "tick_idx = np.arange(0, len(event_years), 3)\n",
    "ax.set_xticks(tick_idx)\n",
    "ax.set_xticklabels(event_years[tick_idx])\n",
    "ax.legend()\n",
    "ax.grid(True, ls=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8884b5f2",
   "metadata": {},
   "source": [
    "# The following scirpt calculte and plot the Mean, Max and Cumulative intensity per year in Arabian sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b732ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculte and plot the Mean, Max and Cumulative intensity per year in Arabian sea\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Zarr file path\n",
    "zarr_path = \"/home/deepak/Desktop/CAS_deepak/Jupyter files/outputs_mhw/arabian_sea/mhw_daily.zarr\"\n",
    "BASELINE = (1982, 2024)\n",
    "ds = xr.open_zarr(zarr_path)\n",
    "\n",
    "# Variables expected from your pipeline\n",
    "mask      = ds[\"mhw_mask\"]      # 1 on event days\n",
    "intensity = ds[\"intensity\"]     # SST - climatology\n",
    "\n",
    "latn = \"lat\" if \"lat\" in ds.coords else \"latitude\"\n",
    "lonn = \"lon\" if \"lon\" in ds.coords else \"longitude\"\n",
    "\n",
    "# Area weights\n",
    "w = np.cos(np.deg2rad(ds[latn]))\n",
    "\n",
    "# Keep only MHW days\n",
    "int_mhw = intensity.where(mask == 1)\n",
    "\n",
    "# Mean intensity per year\n",
    "mean_intensity_year = (int_mhw.groupby(\"time.year\").mean(\"time\").weighted(w).mean(dim=[latn, lonn]).compute())\n",
    "# Max intensity per year\n",
    "max_intensity_year = (int_mhw.groupby(\"time.year\").max(\"time\").weighted(w).mean(dim=[latn, lonn]).compute())\n",
    "\n",
    "\n",
    "# Cumulative intensity per year\n",
    "cum_intensity_year = (int_mhw.groupby(\"time.year\").sum(\"time\").weighted(w).mean(dim=[latn, lonn]).compute())\n",
    "\n",
    "# Extract years\n",
    "years = mean_intensity_year[\"year\"].values.astype(int)\n",
    "\n",
    "\n",
    "# Plotting the bar graphs\n",
    "\n",
    "# Mean intensity\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.bar(years, mean_intensity_year, color=\"#0072B2\", width=0.8)\n",
    "ax.set_title(f\"Mean MHW Intensity in Arabian sea ({BASELINE[0]}–{BASELINE[1]})\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"°C\")\n",
    "tick_idx = np.arange(0, len(years), 5)\n",
    "ax.set_xticks(years[tick_idx])\n",
    "ax.grid(True, ls=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Max intensity\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.bar(years, max_intensity_year, color=\"#D55E00\", width= 0.8)\n",
    "ax.set_title(f\"Maximum MHW Intensity in Arabian sea ({BASELINE[0]}–{BASELINE[1]})\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"°C\")\n",
    "tick_idx = np.arange(0, len(years), 5)\n",
    "ax.set_xticks(years[tick_idx])\n",
    "ax.grid(True, ls=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cumulative intensity\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.bar(years, cum_intensity_year, color=\"#009E73\", width= 0.8)\n",
    "ax.set_title(f\"Cumulative MHW Intensity in Arabian sea ({BASELINE[0]}–{BASELINE[1]})\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"°C · days\")\n",
    "tick_idx = np.arange(0, len(years), 5)\n",
    "ax.set_xticks(years[tick_idx])\n",
    "ax.grid(True, ls=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
